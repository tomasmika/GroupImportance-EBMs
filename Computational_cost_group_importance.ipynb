{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnplAHsl8Fto"
      },
      "source": [
        "# Group Importances\n",
        "In this Python notebook we explore the computational efficiency of EBM group importances compared to more traditional methods such as grouped permutation feature importance (GPFI) (see https://link.springer.com/article/10.1007/s10618-022-00840-5). We compare their runtimes on a variety of OpenML datasets that are popular, open-source and easily accessible, and have different characteristics in terms of number of samples and features. This also allows us to produce results with clean code as little to no data preprocessing is required for these OpenML datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3dFH2nW8w_f"
      },
      "source": [
        "# Installs and imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBYGVvYJ7zx9",
        "outputId": "6b583ef7-a3c7-4a76-9b6b-027dd0a93cb6"
      },
      "outputs": [],
      "source": [
        "!pip3 install interpret numpy pandas openml --quiet\n",
        "!pip3 install --upgrade scikit-learn --quiet\n",
        "\n",
        "!pip install git+https://github.com/lucasplagwitz/grouped_permutation_importance --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OGWqpuHp8E_8",
        "outputId": "2b405e6a-3882-46f2-eb50-3db60ecdc9b1"
      },
      "outputs": [],
      "source": [
        "# Standard\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import openml\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.utils.class_weight import compute_sample_weight\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Imports for Explainable Boosting Machine\n",
        "from interpret.glassbox import ExplainableBoostingClassifier\n",
        "\n",
        "# Group importance methods\n",
        "from interpret.glassbox._ebm._research import * # includes compute_group_importance\n",
        "from grouped_permutation_importance import grouped_permutation_importance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nou-8oJZA20T"
      },
      "source": [
        "# Datasets\n",
        "We pick datasets that have a varying number of samples and features. This enables comparing group importance methods across an array of settings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hfoqP9Ym_k5y"
      },
      "outputs": [],
      "source": [
        "#                                               Samples x Features\n",
        "dataset_ids = [3,     # kr-vs-kp:               3196 x 37\n",
        "               31,    # Credit-g:               1000 x 21\n",
        "               1216,  # Click_prediction_small: 1.5M x 10\n",
        "               1489,  # Phoneme:                5404 x 6\n",
        "               45085, # Breast:                 97 x 24482\n",
        "               45570, # Higgs:                  11M x 29\n",
        "               ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yK1ibl6dCxAs"
      },
      "source": [
        "In this notebook, we are concerned only with the computational efficiency of these methods to compute group importances, not necessarily their interpretations. To that end, we form 5 random groups of roughly equal size for all datasets. The 2 group importance methods take different input, so return both the lists of feature names as well as lists of their indices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nakb7sXWDFpD"
      },
      "outputs": [],
      "source": [
        "def partition(df, num_groups=5):\n",
        "    \"Parition df columns into num_groups groups of roughly equal size.\"\n",
        "    columns = list(enumerate(df.columns))\n",
        "    random.shuffle(columns)\n",
        "    group_size = len(columns)// num_groups\n",
        "\n",
        "    groups = [columns[i * group_size: (i + 1) * group_size] for i in range(num_groups - 1)]\n",
        "    # Add remaining columns to the last group\n",
        "    groups.append(columns[(num_groups - 1) * group_size:])\n",
        "\n",
        "    # Get lists of indices and lists of names for same groupings\n",
        "    index_groups = [[index for index, _ in group] for group in groups]\n",
        "    name_groups = [[col_name for _, col_name in group] for group in groups]\n",
        "\n",
        "    return index_groups, name_groups"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcGwiaEFC96i"
      },
      "source": [
        "# Computational Cost of Group Importances\n",
        "For every dataset, get X and y and build the classifier, an EBM in this case. Then, acquire random groupings and compute group importances using (1) EBM's internal method for group importances, and (2) Grouped permutation feature importance (GPFI) as defined in https://link.springer.com/article/10.1007/s10618-022-00840-5."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-CXxpzomA174",
        "outputId": "e907a265-430c-49b4-95ff-ff43a8495960"
      },
      "outputs": [],
      "source": [
        "# Keep track of runtimes\n",
        "EBM_runtime = []\n",
        "GPFI_runtime = []\n",
        "\n",
        "for id in tqdm(dataset_ids):\n",
        "    # Pull dataset\n",
        "    print(\"Dataset ID:\", id)\n",
        "    openml_dataset = openml.datasets.get_dataset(id)\n",
        "    X, y, _, _ = openml_dataset.get_data(target=openml_dataset.default_target_attribute)\n",
        "    w = compute_sample_weight(class_weight=\"balanced\", y=y) # Sample weights\n",
        "\n",
        "    # Build classifier\n",
        "    model = ExplainableBoostingClassifier(max_bins=128, max_rounds=5000, smoothing_rounds=500, outer_bags=8)\n",
        "    model.fit(X, y, sample_weight=w)\n",
        "\n",
        "    # Compute random groupings\n",
        "    idxs, names = partition(X, num_groups=5)\n",
        "\n",
        "    # Compute group importances using EBM's group importance method. Track runtime\n",
        "    t1_EBM = time.perf_counter()\n",
        "    for g in names: # EBMs compute group importance one by one, so loop over them\n",
        "        compute_group_importance(g, model, X)\n",
        "    t2_EBM = time.perf_counter()\n",
        "    EBM_runtime.append(t2_EBM - t1_EBM) # Track EBM runtime\n",
        "    print(\"EBM:\", t2_EBM - t1_EBM)\n",
        "\n",
        "    # Compute group importances using GPFI: GPFI implements it with all groups together in idxs, so only one call is needed.\n",
        "    t1_GPFI = time.perf_counter()\n",
        "    r_GPFI = grouped_permutation_importance(model, X.values, y.values, idxs=idxs, n_repeats=5, sample_weight=compute_sample_weight(class_weight=\"balanced\", y=y))\n",
        "    t2_GPFI = time.perf_counter()\n",
        "    GPFI_runtime.append(t2_GPFI - t1_GPFI) # Track GPFI runtime\n",
        "    print(\"GPFI:\", t2_GPFI - t1_GPFI, '\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D1N39ddfHoEM"
      },
      "outputs": [],
      "source": [
        "runtime_df = pd.DataFrame({'EBM Runtime': EBM_runtime, \"GPFI Runtime\": GPFI_runtime, \"Dataset ID\": dataset_ids})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eg5JDJjG-xJT"
      },
      "outputs": [],
      "source": [
        "runtime_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CX_ZPsTKMoqd"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
